# -*- coding: utf-8 -*-
import scrapy
import re
from somenew.items import SomenewItem
import datetime
import hashlib
from copy import deepcopy
import json
from elasticsearch import Elasticsearch
import importlib
import sys
importlib.reload(sys)

class HuanqiuSpider(scrapy.Spider):
    name = 'huanqiu'
    allowed_domains = ['huanqiu.com']
    # 从es中获取一个星期之内的，环球网所有的url。生成start_urls列表
    es = Elasticsearch(hosts=['192.168.3.15'])
    # 查询结束时间
    this_day = datetime.date.today()
    tomorrow = this_day + datetime.timedelta(days=1)
    end_day = str(tomorrow) + 'T00:00:00'
    # 查询起始时间
    start = this_day - datetime.timedelta(days=7)
    start_day = str(start) + 'T00:00:00'

    query_json = {
        "bool": {"must": [
            {"match": {
                "media": "环球网"
            }}
        ],
            "filter": {"range": {
                "publish_time": {
                    "gte": start_day,
                    "lt": end_day}
            }
            }
        }
    }
    # 获取总数["hits"]["total"]
    res = es.count(index="spider", doc_type='article', body={"query": query_json})
    res2 = es.search(index="spider", doc_type='article', body={"query": query_json, "size": res['count']})
    urls = []
    for i in res2['hits']['hits']:
        urls.append(i['_source']['url'])
    start_urls = urls

    def parse(self, response):
        item = SomenewItem()
        url = str(response.url)
        title = response.xpath("//div[@class='l_a']/h1/text()").extract_first()
        m = hashlib.md5()
        m.update(str(url).encode('utf8'))
        article_id = str(m.hexdigest())
        item['article_id'] = article_id
        sourceid = response.xpath("//meta[@name='contentid']/@content").extract_first()
        com_url = 'https://commentn.huanqiu.com/api/v2/async?a=comment&m=source_info&appid=e8fcff106c8f&sourceid=' + sourceid + '&url=' + \
                  url + '&title=' + title
        yield scrapy.Request(com_url, callback=self.get_com_num, meta={'item': item})

    def get_com_num(self, response):
        item = deepcopy(response.meta['item'])
        ret = response.body.decode()
        dict = json.loads(ret)
        if dict['msg'] == 'success':
            n_comment = dict['data']['n_comment']
            n_active = dict['data']['n_active']
            item['comm_num'] = int(n_comment) + int(n_active)
        else:
            item['comm_num'] = '0'
        item['fav_num'] = '0'
        item['read_num'] = '0'
        item['env_num'] = '0'
        item['hot_value'] = item['comm_num']
        yield item


    # def get_content(self,response):
    #     item=SomenewItem()
    #     item['title'] = response.xpath("//div[@class='l_a']/h1/text()").extract_first()
    #     item['time'] = response.xpath("//div[@class='la_tool']/span/text()").extract_first()
    #     item['url'] = response.url
    #     item['content'] = response.xpath("//div[@class='la_con']/p//text()").extract()
    #     item['content'] = ''.join(item["content"]).replace(u'\u3000', u' ').replace(u'\xa0', u' ')
    #     item['media'] = '环球网'
    #     item['create_time'] = datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S')
    #     m = hashlib.md5()
    #     url = str(item['url'])
    #     m.update(str(url).encode('utf8'))
    #     article_id = str(m.hexdigest())
    #     item['article_id'] = article_id
    #     # https://commentn.huanqiu.com/api/v2/async?a=comment&m=source_info&appid=e8fcff106c8f&sourceid=12431395&url=http://china.huanqiu.com/article/2018-07/12431395.html&title=30多万人疯狂购买、涉案金额50亿！起底新型传销骗局
    #     sourceid= response.xpath("//meta[@name='contentid']/@content").extract_first()
    #     com_url = 'https://commentn.huanqiu.com/api/v2/async?a=comment&m=source_info&appid=e8fcff106c8f&sourceid='+sourceid+'&url='+item['url']+'&title='+item['title']
    #     yield scrapy.Request(com_url,callback=self.get_com_num,meta={'item': item})
    #
    # def get_com_num(self,response):
    #     item = deepcopy(response.meta['item'])
    #     ret = response.body.decode()
    #     dict = json.loads(ret)
    #     if dict['msg'] == 'success':
    #         n_comment = dict['data']['n_comment']
    #         n_active = dict['data']['n_active']
    #         item['comm_num'] = int(n_comment) + int(n_active)
    #     else:
    #         item['comm_num'] = '0'
    #     item['fav_num'] = '0'
    #     item['read_num'] = '0'
    #     item['env_num'] = '0'
    #     yield item



