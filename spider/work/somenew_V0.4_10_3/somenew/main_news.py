# coding=utf-8

import time
import os

# while True:
def run_spider():
    # os.system("scrapy crawl bjqingnian")
    # print("0" * 100)
    #
    # os.system("scrapy crawl cctv_jj")
    # print("1" * 100)
    #
    # os.system("scrapy crawl chinanews")
    # print("2" * 100)
    #
    # os.system("scrapy crawl cnmo")
    # print("3" * 100)
    #
    # os.system("scrapy crawl dazhewang")
    # print("4" * 100)
    #
    os.system("scrapy crawl xizang_ribao")        # start_urls已经无效,故爬虫需要重写
    # print("5 * 100")
    #
    # os.system("scrapy crawl gjjr")
    # print("6" * 100)
    #
    # os.system("scrapy crawl hangzhouwang")
    # print("7" * 100)
    #
    # os.system("scrapy crawl hschenbao")     # 华商晨报休刊,网站打不开
    # print("9" * 100)
    #
    # os.system("scrapy crawl huanqiu")
    # print("8" * 100)
    #
    # os.system("scrapy crawl jiangxifayuan")
    # print("9" * 100)
    #
    # os.system("scrapy crawl jingjiribao")
    # print("10" * 100)
    #
    # os.system("scrapy crawl laodong")
    # print("11" * 100)
    #
    # os.system("scrapy crawl pengpai")
    # print("12" * 100)
    #
    # os.system("scrapy crawl peoplePaper")
    # print("13" * 100)
    #
    #
    # os.system("scrapy crawl qqnews")     # 获取不到数据,需要重新更新提取规则
    # print("14" * 100)
    #
    # os.system("scrapy crawl sina_leju")
    # print("15" * 100)
    #
    # os.system("scrapy crawl xinhua")
    # print("17" * 100)
    #
    # os.system("scrapy crawl xinjingbao")
    # print("18" * 100)
    #
    # os.system("scrapy crawl yicai")
    # print("19" * 100)
    #
    # os.system("scrapy crawl zgjingji")
    # print("20" * 100)
    #
    # os.system("scrapy crawl zhejiangzaixian")    # 未获取到数据,玉林已经写了
    # print("21" * 100)
    #
    # os.system("scrapy crawl zhongzheng")
    # print("22" * 100)

    # os.system("scrapy crawl zqrb_cj")
    # print("23" * 100)

    # os.system("scrapy crawl wangyi2")
    # print("23" * 100)

if __name__ == '__main__':
    run_spider()
