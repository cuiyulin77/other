# coding=utf8
import jieba
import pandas as pd
from elasticsearch import Elasticsearch
import datetime
import jieba.analyse as ana
import os

def get_words_weight(this_today, tomorrow_day, es, key_list):
    # this_day = datetime.date.today()
    # this_day = '2018-07-15'
    # this_today = str(this_day) + 'T00:00:00'
    # this_today = '2018-07-05' + 'T00:00:00'
    # 查询结束时间
    # tomorrow = this_day + datetime.timedelta(days=1)
    # tomorrow_day = str(tomorrow) + 'T00:00:00'
    #
    # es = Elasticsearch(hosts=['192.168.3.15'])
    # key_list = ['意识形态', '公共文化', '文明创建']
    query_json = {
        "bool": {
            "must": [
                {"terms": {
                    "content": key_list
                }}
            ], "filter": [{"range": {"publish_time": {
                "gte": this_today,
                "lt": tomorrow_day}}}]
        }
    }

    res = es.search(index="spider", doc_type='article', body={"query": query_json, "size": 0})
    res_num = res["hits"]["total"]
    print(res_num)
    txts = es.search(index='spider', doc_type='article', body={'query': query_json, 'size': res_num})
    hits = txts['hits']['hits']
    contents = []
    for hit in hits:
        # print(hit['_source']['content'],'\n')
        contents.append(hit['_source']['content'])
    # print(''.join(contents))

    words_str = ''.join(contents).replace("\n", '').replace(" ", '')

    txt_path = os.path.dirname(os.path.abspath(__file__))+'/停用词.txt'
    ana.set_stop_words(txt_path)
    words_list = ana.extract_tags(words_str, topK=20, withWeight=True)
    print(len(words_list))
    print(words_list)
    # for v,n in words_list:
    #     print(v + '\t' + str(int(n * 10000)))
    return words_list

if __name__ == '__main__':
    txt_path = os.path.dirname(os.path.abspath(__file__))+'/停用词.txt'
    print(txt_path)


